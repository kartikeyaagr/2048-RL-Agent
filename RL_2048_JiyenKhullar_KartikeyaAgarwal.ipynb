{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 1: Game Environment Implementation\n",
    "First, we need to create a 2048 game environment that will:\n",
    "\n",
    "Maintain the game state\n",
    "Process moves\n",
    "Calculate rewards\n",
    "Determine game termination\"\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class Game2048:\n",
    "    def __init__(self):\n",
    "        # Initialize 4x4 grid with zeros\n",
    "        self.board = np.zeros((4, 4), dtype=int)\n",
    "        self.score = 0\n",
    "        self.game_over = False\n",
    "        # Add initial tiles\n",
    "        self.add_random_tile()\n",
    "        self.add_random_tile()\n",
    "    \n",
    "    def reset(self):\n",
    "        # Reset the game to initial state\n",
    "        self.__init__()\n",
    "        return self.get_state()\n",
    "    \n",
    "    def add_random_tile(self):\n",
    "        # Find all empty cells\n",
    "        empty_cells = np.where(self.board == 0)\n",
    "        if len(empty_cells[0]) > 0:\n",
    "            # Choose a random empty cell\n",
    "            idx = random.randint(0, len(empty_cells[0]) - 1)\n",
    "            row, col = empty_cells[0][idx], empty_cells[1][idx]\n",
    "            # Place a 2 (90% chance) or 4 (10% chance)\n",
    "            self.board[row, col] = 2 if random.random() < 0.9 else 4\n",
    "    \n",
    "    def move(self, direction):\n",
    "        # 0: left, 1: up, 2: right, 3: down\n",
    "        # Save original state for comparison\n",
    "        original_board = self.board.copy()\n",
    "        original_score = self.score\n",
    "        \n",
    "        # Move tiles based on direction\n",
    "        if direction == 0:  # Left\n",
    "            self.board, score_increase = self._move_left(self.board)\n",
    "        elif direction == 1:  # Up\n",
    "            self.board = np.rot90(self.board, k=1)\n",
    "            self.board, score_increase = self._move_left(self.board)\n",
    "            self.board = np.rot90(self.board, k=-1)\n",
    "        elif direction == 2:  # Right\n",
    "            self.board = np.rot90(self.board, k=2)\n",
    "            self.board, score_increase = self._move_left(self.board)\n",
    "            self.board = np.rot90(self.board, k=-2)\n",
    "        elif direction == 3:  # Down\n",
    "            self.board = np.rot90(self.board, k=-1)\n",
    "            self.board, score_increase = self._move_left(self.board)\n",
    "            self.board = np.rot90(self.board, k=1)\n",
    "        \n",
    "        # Update score\n",
    "        self.score += score_increase\n",
    "        \n",
    "        # Check if board changed\n",
    "        moved = not np.array_equal(self.board, original_board)\n",
    "        \n",
    "        # Add new tile if board changed\n",
    "        if moved:\n",
    "            self.add_random_tile()\n",
    "        \n",
    "        # Check if game is over\n",
    "        self.game_over = self._is_game_over()\n",
    "        \n",
    "        return moved, score_increase, self.game_over\n",
    "    \n",
    "    def _move_left(self, board):\n",
    "        new_board = np.zeros_like(board)\n",
    "        score_increase = 0\n",
    "        \n",
    "        for row in range(4):\n",
    "            # Extract non-zero elements\n",
    "            elements = board[row][board[row] != 0]\n",
    "            merged = []\n",
    "            i = 0\n",
    "            \n",
    "            # Merge adjacent identical tiles\n",
    "            while i < len(elements):\n",
    "                if i + 1 < len(elements) and elements[i] == elements[i + 1]:\n",
    "                    merged.append(elements[i] * 2)\n",
    "                    score_increase += elements[i] * 2\n",
    "                    i += 2\n",
    "                else:\n",
    "                    merged.append(elements[i])\n",
    "                    i += 1\n",
    "            \n",
    "            # Place merged elements in new board\n",
    "            new_board[row, :len(merged)] = merged\n",
    "        \n",
    "        return new_board, score_increase\n",
    "    \n",
    "    def _is_game_over(self):\n",
    "        # Check if there are any empty cells\n",
    "        if np.any(self.board == 0):\n",
    "            return False\n",
    "        \n",
    "        # Check if there are any adjacent identical tiles\n",
    "        for row in range(4):\n",
    "            for col in range(4):\n",
    "                current = self.board[row, col]\n",
    "                # Check right\n",
    "                if col < 3 and current == self.board[row, col + 1]:\n",
    "                    return False\n",
    "                # Check down\n",
    "                if row < 3 and current == self.board[row + 1, col]:\n",
    "                    return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def get_state(self):\n",
    "        # Return the current state representation\n",
    "        return self.board.copy()\n",
    "    \n",
    "    def get_valid_actions(self):\n",
    "        # Return list of valid actions\n",
    "        valid_actions = []\n",
    "        for direction in range(4):\n",
    "            # Test if move is valid (board changes)\n",
    "            test_board = self.board.copy()\n",
    "            if direction == 0:  # Left\n",
    "                result, _ = self._move_left(test_board)  # Only unpack 2 values\n",
    "                if not np.array_equal(result, test_board):\n",
    "                    valid_actions.append(0)\n",
    "            elif direction == 1:  # Up\n",
    "                test_board = np.rot90(test_board, k=1)\n",
    "                result, _ = self._move_left(test_board)  # Only unpack 2 values\n",
    "                if not np.array_equal(result, np.rot90(test_board, k=-1)):\n",
    "                    valid_actions.append(1)\n",
    "            elif direction == 2:  # Right\n",
    "                test_board = np.rot90(test_board, k=2)\n",
    "                result, _ = self._move_left(test_board)  # Only unpack 2 values\n",
    "                if not np.array_equal(result, np.rot90(test_board, k=-2)):\n",
    "                    valid_actions.append(2)\n",
    "            elif direction == 3:  # Down\n",
    "                test_board = np.rot90(test_board, k=-1)\n",
    "                result, _ = self._move_left(test_board)  # Only unpack 2 values\n",
    "                if not np.array_equal(result, np.rot90(test_board, k=1)):\n",
    "                    valid_actions.append(3)\n",
    "        \n",
    "        return valid_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 2: State Representation\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from math import log2\n",
    "from numpy import array, zeros\n",
    "\n",
    "def transform_game_state(game_board):\n",
    "    # Flatten and prepare the board data\n",
    "    flattened = array(game_board).ravel()\n",
    "    \n",
    "    # Transform values using logarithm base 2\n",
    "    transformed_values = []\n",
    "    for cell in flattened:\n",
    "        transformed_values.append(0 if cell == 0 else int(log2(cell)))\n",
    "    \n",
    "    # Create tensor from transformed values\n",
    "    state_tensor = torch.tensor(transformed_values, dtype=torch.int64)\n",
    "    \n",
    "    # Create one-hot representation\n",
    "    expanded_tensor = F.one_hot(state_tensor, num_classes=16).to(torch.float32)\n",
    "    \n",
    "    # Reshape into format suitable for convolutional networks: [batch, channels, height, width]\n",
    "    reshaped_tensor = expanded_tensor.view(1, 16, 4, 4)\n",
    "    \n",
    "    return reshaped_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 3: Deep Q-Network Architecture\n",
    "\"\"\"\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(Block, self).__init__()\n",
    "        # Each filter size gets 1/4 of channels\n",
    "        c = out_ch // 4\n",
    "        self.c1 = nn.Conv2d(in_ch, c, 1, padding='same')\n",
    "        self.c2 = nn.Conv2d(in_ch, c, 2, padding='same')\n",
    "        self.c3 = nn.Conv2d(in_ch, c, 3, padding='same')\n",
    "        self.c4 = nn.Conv2d(in_ch, c, 4, padding='same')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Join outputs from different filters\n",
    "        o1 = self.c1(x)\n",
    "        o2 = self.c2(x)\n",
    "        o3 = self.c3(x)\n",
    "        o4 = self.c4(x)\n",
    "        return torch.cat([o1, o2, o3, o4], 1)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        # Layers\n",
    "        self.b1 = Block(16, 128)\n",
    "        self.b2 = Block(128, 256)\n",
    "        self.b3 = Block(256, 512)\n",
    "        \n",
    "        self.l1 = nn.Linear(512 * 16, 256)\n",
    "        self.l2 = nn.Linear(256, 4)  # 4 actions\n",
    "        \n",
    "        self.act = nn.ReLU()\n",
    "        self.drop = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Process through blocks\n",
    "        x = self.act(self.b1(x))\n",
    "        x = self.act(self.b2(x))\n",
    "        x = self.act(self.b3(x))\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Process through linear layers\n",
    "        x = self.drop(self.act(self.l1(x)))\n",
    "        x = self.l2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 4: Experience Replay Buffer\n",
    "\"\"\"\n",
    "    \n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "\n",
    "# Define a data structure for experiences\n",
    "Step = namedtuple('Step', \n",
    "                ('s', 'a', 'next_s', 'r', 'done'))\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self, size):\n",
    "        self.buffer = deque(maxlen=size)\n",
    "    \n",
    "    def add(self, s, a, next_s, r, done):\n",
    "        # Store new experience\n",
    "        self.buffer.append(Step(s, a, next_s, r, done))\n",
    "    \n",
    "    def get_batch(self, size):\n",
    "        # Return random batch\n",
    "        return random.sample(self.buffer, size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 5: Model Agent\n",
    "\"\"\"\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, \n",
    "                 buffer_size=100000, batch_size=64, gamma=0.99,\n",
    "                 learning_rate=1e-4, update_every=5):\n",
    "        # Initialize parameters\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.update_every = update_every\n",
    "        \n",
    "        # Device configuration\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Q-Networks\n",
    "        self.policy_net = Model().to(self.device)\n",
    "        self.target_net = Model().to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()  # Set to evaluation mode\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Replay buffer\n",
    "        self.memory = Memory(buffer_size)\n",
    "        \n",
    "        # Initialize exploration parameters\n",
    "        self.eps_start = 0.9\n",
    "        self.eps_end = 0.05\n",
    "        self.eps_decay = 0.9999\n",
    "        self.eps = self.eps_start\n",
    "        \n",
    "        # Training step counter\n",
    "        self.t_step = 0\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, next_state, reward, done)\n",
    "        \n",
    "        # Increment step counter\n",
    "        self.t_step += 1\n",
    "        \n",
    "        # If enough samples in memory, perform learning\n",
    "        if len(self.memory) > self.batch_size and self.t_step % self.update_every == 0:\n",
    "            experiences = self.memory.get_batch(self.batch_size)\n",
    "            self.learn(experiences)\n",
    "    \n",
    "    def act(self, state, valid_actions=None):\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > self.eps:\n",
    "            # Check if state is already a tensor\n",
    "            if not isinstance(state, torch.Tensor):\n",
    "                state = torch.from_numpy(state).float().to(self.device)\n",
    "            else:\n",
    "                # Make sure it's on the right device\n",
    "                state = state.to(self.device)\n",
    "                \n",
    "            self.policy_net.eval()\n",
    "            with torch.no_grad():\n",
    "                action_values = self.policy_net(state)\n",
    "            self.policy_net.train()\n",
    "            \n",
    "            # If valid actions provided, only consider those\n",
    "            if valid_actions:\n",
    "                mask = torch.ones_like(action_values) * float('-inf')\n",
    "                mask[0, valid_actions] = 0\n",
    "                action_values = action_values + mask\n",
    "            \n",
    "            return action_values.cpu().data.numpy().argmax()\n",
    "        else:\n",
    "            # Choose random action\n",
    "            return random.choice(valid_actions) if valid_actions else random.randint(0, self.action_size-1)\n",
    "        \n",
    "    def learn(self, experiences):\n",
    "        # Unpack experiences\n",
    "        states = torch.cat([e.s for e in experiences]).to(self.device)\n",
    "        actions = torch.tensor([e.a for e in experiences], device=self.device).unsqueeze(1)\n",
    "        rewards = torch.tensor([e.r for e in experiences], device=self.device).unsqueeze(1)\n",
    "        next_states = torch.cat([e.next_s for e in experiences]).to(self.device)\n",
    "        dones = torch.tensor([e.done for e in experiences], device=self.device).unsqueeze(1).float()\n",
    "        \n",
    "        # Get current Q values\n",
    "        Q_expected = self.policy_net(states).gather(1, actions)\n",
    "        \n",
    "        # Get next Q values from target network\n",
    "        with torch.no_grad():\n",
    "            Q_targets_next = self.target_net(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        \n",
    "        # Compute target Q values\n",
    "        Q_targets = rewards + (self.gamma * Q_targets_next * (1 - dones))\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        \n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update target network\n",
    "        if self.t_step % self.update_every == 0:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        \n",
    "        # Update epsilon\n",
    "        self.eps = max(self.eps_end, self.eps * self.eps_decay)\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def save(self, filename):\n",
    "        torch.save({\n",
    "            'policy_net_state_dict': self.policy_net.state_dict(),\n",
    "            'target_net_state_dict': self.target_net.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'epsilon': self.eps,\n",
    "        }, filename)\n",
    "    \n",
    "    def load(self, filename):\n",
    "        checkpoint = torch.load(filename)\n",
    "        self.policy_net.load_state_dict(checkpoint['policy_net_state_dict'])\n",
    "        self.target_net.load_state_dict(checkpoint['target_net_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.eps = checkpoint['epsilon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 6: Training Loop\n",
    "\"\"\"\n",
    "\n",
    "def train_agent(num_episodes=10000, max_steps=10000):\n",
    "    # Initialize environment and agent\n",
    "    env = Game2048()\n",
    "    agent = DQNAgent(state_size=16, action_size=4)\n",
    "    \n",
    "    # Training statistics\n",
    "    scores = []\n",
    "    max_tiles = []\n",
    "    \n",
    "    for episode in range(1, num_episodes+1):\n",
    "        # Reset environment\n",
    "        state = env.reset()\n",
    "        state = transform_game_state(state)\n",
    "        score = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Get valid actions\n",
    "            valid_actions = env.get_valid_actions()\n",
    "            \n",
    "            # If no valid actions, game is over\n",
    "            if not valid_actions:\n",
    "                break\n",
    "            \n",
    "            # Select action\n",
    "            action = agent.act(state, valid_actions)\n",
    "            \n",
    "            # Take action\n",
    "            moved, reward, done = env.move(action)\n",
    "            \n",
    "            # If move didn't change board, assign negative reward\n",
    "            if not moved:\n",
    "                reward = -1\n",
    "            \n",
    "            # Get next state\n",
    "            next_state = env.get_state()\n",
    "            next_state = transform_game_state(next_state)\n",
    "            \n",
    "            # Update agent\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Update state and score\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Record statistics\n",
    "        scores.append(score)\n",
    "        max_tiles.append(env.board.max())\n",
    "        \n",
    "        # Print progress\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode {episode}/{num_episodes}, Avg Score: {sum(scores[-100:])/100:.2f}, Max Tile: {max(max_tiles[-100:])}\")\n",
    "            # Save model\n",
    "            agent.save(f\"dqn_2048_episode_{episode}.pth\")\n",
    "    \n",
    "    return agent, scores, max_tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 7: Evaluation Loop\n",
    "\"\"\"\n",
    "def evaluate_agent(agent, num_games=100, max_steps=1000):\n",
    "    env = Game2048()\n",
    "    scores = []\n",
    "    max_tiles = []\n",
    "    \n",
    "    for i in range(num_games):\n",
    "        state = env.reset()\n",
    "        state = transform_game_state(state)\n",
    "        done = False\n",
    "        total_score = 0\n",
    "        step_count = 0\n",
    "        \n",
    "        while not done and step_count < max_steps:\n",
    "            # Get valid actions\n",
    "            valid_actions = env.get_valid_actions()\n",
    "            \n",
    "            # If no valid actions, game is over\n",
    "            if not valid_actions:\n",
    "                break\n",
    "            \n",
    "            # Select action (no exploration)\n",
    "            agent.eps = 0\n",
    "            \n",
    "            # Try actions until a valid one is found\n",
    "            moved = False\n",
    "            max_attempts = 10  # Limit the number of attempts to find a valid move\n",
    "            attempts = 0\n",
    "            \n",
    "            while not moved and attempts < max_attempts and valid_actions:\n",
    "                action = agent.act(state, valid_actions)\n",
    "                \n",
    "                # Take action\n",
    "                moved, reward, done = env.move(action)\n",
    "                \n",
    "                # If move didn't change board, remove this action and try again\n",
    "                if not moved:\n",
    "                    if action in valid_actions:\n",
    "                        valid_actions.remove(action)\n",
    "                    attempts += 1\n",
    "                    # Restore the board state since the move was invalid\n",
    "                    env.board = env.board.copy()  # This might not be necessary if env.move() already restores the state\n",
    "                else:\n",
    "                    # Valid move found, update state and score\n",
    "                    state = env.get_state()\n",
    "                    state = transform_game_state(state)\n",
    "                    total_score += reward\n",
    "                    step_count += 1\n",
    "            \n",
    "            # If we couldn't find a valid move after max attempts, end the game\n",
    "            if not moved:\n",
    "                break\n",
    "        \n",
    "        scores.append(total_score)\n",
    "        max_tiles.append(env.board.max())\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f\"Game {i}/{num_games}, Score: {total_score}, Max Tile: {env.board.max()}, Steps: {step_count}\")\n",
    "    \n",
    "    print(f\"Average Score: {sum(scores)/len(scores):.2f}\")\n",
    "    print(f\"Average Max Tile: {sum(max_tiles)/len(max_tiles):.2f}\")\n",
    "    \n",
    "    return scores, max_tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 8: Train Agent\n",
    "\"\"\"\n",
    "\n",
    "# Train agent\n",
    "agent, train_scores, train_max_tiles = train_agent(num_episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 9: Evaluate Agent\n",
    "\"\"\"\n",
    "# Evaluate agent\n",
    "print(\"Eval\")\n",
    "eval_scores, eval_max_tiles = evaluate_agent(agent, num_games=100)\n",
    "\n",
    "# Plot results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training scores\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_scores)\n",
    "plt.title('Training Scores')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Score')\n",
    "plt.savefig('training_scores.png')\n",
    "\n",
    "# Plot max tiles during training\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_max_tiles)\n",
    "plt.title('Max Tiles During Training')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Max Tile')\n",
    "plt.savefig('training_max_tiles.png')\n",
    "\n",
    "# Plot evaluation score distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(eval_scores, bins=20)\n",
    "plt.title('Evaluation Score Distribution')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig('eval_score_dist.png')\n",
    "\n",
    "# Plot evaluation max tile distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "unique, counts = np.unique(eval_max_tiles, return_counts=True)\n",
    "plt.bar([str(int(x)) for x in unique], counts)\n",
    "plt.title('Evaluation Max Tile Distribution')\n",
    "plt.xlabel('Max Tile')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig('eval_max_tile_dist.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
